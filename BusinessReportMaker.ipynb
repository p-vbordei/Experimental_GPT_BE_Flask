{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### REPORT BUILDER ####\n",
    "# !pip install feedparser\n",
    "#!pip install spacy\n",
    "#!pip install transformers\n",
    "#!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import requests\n",
    "import spacy\n",
    "import feedparser\n",
    "\n",
    "\n",
    "class PreprocessorModule:\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    def generate_search_queries(self, topic):\n",
    "            # Step 1: Input Processing\n",
    "            topic = topic.lower() # convert the text to lowercase\n",
    "            print(f\"Step 1: Lowercase topic: {topic}\")\n",
    "            topic_tokens = word_tokenize(topic)\n",
    "\n",
    "            # Step 2: Part-of-Speech (POS) Tagging\n",
    "            topic_pos_tags = nltk.pos_tag(topic_tokens)\n",
    "            print(f\"Step 2: POS tags: {topic_pos_tags}\")\n",
    "\n",
    "            # Step 3: Named Entity Recognition (NER)\n",
    "            # Implement a NER algorithm of your choice here to identify named entities in the topic\n",
    "\n",
    "            # Step 4: Stopword Removal\n",
    "            stop_words = set(stopwords.words('english'))\n",
    "            topic_filtered_tokens = [word for word in topic_tokens if not word in stop_words]\n",
    "            print(f\"Step 4: Filtered tokens (stopwords removed): {topic_filtered_tokens}\")\n",
    "\n",
    "            # Step 5: Stemming\n",
    "            ps = PorterStemmer()\n",
    "            topic_stemmed_tokens = [ps.stem(word) for word in topic_filtered_tokens]\n",
    "            print(f\"Step 5: Stemmed tokens: {topic_stemmed_tokens}\")\n",
    "\n",
    "            # Step 6: Query Generation\n",
    "            search_query = \" \".join(topic_filtered_tokens)\n",
    "            print(f\"Step 6: Search query: {search_query}\")\n",
    "\n",
    "            # Step 7: Query Refinement\n",
    "            for i in range(3):\n",
    "                # Ask the user to provide feedback on the initial search results\n",
    "                feedback = input(\"Please provide feedback on the initial search results: \")\n",
    "                \n",
    "                # If the user is satisfied with the results, return the search queries\n",
    "                if feedback == \"satisfied\":\n",
    "                    return [search_query]\n",
    "\n",
    "            # If the user is still not satisfied after three rounds of refinement, return the original search queries\n",
    "            print(f\"User not satisfied after 3 rounds. Returning original search query: {search_query}\")\n",
    "            return [search_query]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from scholarly import scholarly\n",
    "\n",
    "class DataRetrievalModule:\n",
    "\n",
    "    def search_google_scholar(self, query):\n",
    "        # Add code to search using the Google Scholar API\n",
    "        search_results = []\n",
    "        try:\n",
    "            print(f\"Searching Google Scholar with query: {query}\")\n",
    "            search_query = scholarly.search_pubs(query.encode(\"utf-8\"))\n",
    "            print(f\"Google Scholar search query: {search_query}\")\n",
    "            for result in search_query:\n",
    "                search_results.append({\n",
    "                    \"title\": result[\"bib\"][\"title\"],\n",
    "                    \"url\": result[\"pub_url\"],\n",
    "                    \"authors\": result[\"bib\"].get(\"author\", \"\"),\n",
    "                    \"abstract\": result[\"bib\"].get(\"abstract\", \"\"),\n",
    "                    \"publication_date\": result[\"bib\"].get(\"pub_year\", \"\"),\n",
    "                    \"source\": \"Google Scholar\"\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error while searching Google Scholar: {e}\")\n",
    "        print(f\"Google Scholar search results: {search_results}\")\n",
    "        return search_results\n",
    "\n",
    "    def search_arxiv(self, query):\n",
    "        base_url = \"http://export.arxiv.org/api/query\"\n",
    "        params = {\n",
    "            \"search_query\": f\"all:{query}\",\n",
    "            \"start\": 0,\n",
    "            \"max_results\": 10,\n",
    "            \"sortBy\": \"relevance\",\n",
    "            \"sortOrder\": \"descending\"\n",
    "        }\n",
    "        print(f\"Searching ArXiv with query: {query}\")\n",
    "        print(f\"ArXiv search parameters: {params}\")\n",
    "        response = requests.get(base_url, params=params)\n",
    "        feed = feedparser.parse(response.content)\n",
    "\n",
    "        results = []\n",
    "        for entry in feed.entries:\n",
    "            results.append({\n",
    "                \"title\": entry[\"title\"],\n",
    "                \"url\": entry[\"link\"],\n",
    "                \"authors\": [author[\"name\"] for author in entry[\"authors\"]],\n",
    "                \"publication_date\": entry[\"published\"],\n",
    "                \"source\": \"ArXiv\"\n",
    "            })\n",
    "        print(f\"ArXiv search results: {results}\")\n",
    "        return results\n",
    "\n",
    "    def retrieve_data(self, query):\n",
    "        results = []\n",
    "        print(f\"Retrieving data with query: {query}\")\n",
    "        results.extend(self.search_google_scholar(query))\n",
    "        results.extend(self.search_arxiv(query))\n",
    "        filtered_results = self.filter_and_sort(results)\n",
    "        print(f\"Filtered and sorted results: {filtered_results}\")\n",
    "        return filtered_results\n",
    "\n",
    "    def filter_and_sort(self, results):\n",
    "        # Filter out results without a publication date\n",
    "        filtered_results = [r for r in results if r[\"publication_date\"]]\n",
    "        print(f\"Filtering results without a publication date: {filtered_results}\")\n",
    "\n",
    "        # Calculate a quality score based on the number of authors\n",
    "        for r in filtered_results:\n",
    "            r[\"quality_score\"] = len(r[\"authors\"])\n",
    "        print(f\"Calculating quality score for each result: {filtered_results}\")\n",
    "\n",
    "        # Sort the results by the quality score, in descending order\n",
    "        sorted_results = sorted(filtered_results, key=lambda x: x[\"quality_score\"], reverse=True)\n",
    "        print(f\"Sorted results by quality score: {sorted_results}\")\n",
    "\n",
    "        return sorted_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from transformers import pipeline\n",
    "\n",
    "class DataAnalysisModule:\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.summarizer = pipeline(\"summarization\")\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        # Add code to preprocess the text, e.g., remove special characters or extra whitespace\n",
    "        processed_text = text.strip()\n",
    "        print(f\"Preprocessed text: {processed_text}\")\n",
    "        return processed_text\n",
    "\n",
    "    def extract_entities(self, text):\n",
    "        # Add code to extract named entities using the NLP library\n",
    "        doc = self.nlp(text)\n",
    "        entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "        print(f\"Extracted entities: {entities}\")\n",
    "        return entities\n",
    "\n",
    "    def extract_keywords(self, text):\n",
    "        # Add code to extract keywords and phrases from the text\n",
    "        doc = self.nlp(text)\n",
    "        keywords = [chunk.text for chunk in doc.noun_chunks]\n",
    "        print(f\"Extracted keywords: {keywords}\")\n",
    "        return keywords\n",
    "\n",
    "    def summarize(self, text):\n",
    "        # Add code to generate a summary using the Hugging Face Transformers library\n",
    "        summary = self.summarizer(text, max_length=2000, min_length=500, do_sample=False)\n",
    "        print(f\"Generated summary: {summary[0]['summary_text']}\")\n",
    "        return summary[0]['summary_text']\n",
    "\n",
    "    def paraphrase(self, text):\n",
    "        # Add code to paraphrase the text using an appropriate NLP technique or model\n",
    "        # Note: Paraphrasing is a complex task and requires a dedicated model.\n",
    "        # For now, the function returns the input text.\n",
    "        paraphrased_text = text\n",
    "        print(f\"Paraphrased text (not implemented): {paraphrased_text}\")\n",
    "        return paraphrased_text\n",
    "\n",
    "    def analyze_data(self, data):\n",
    "        processed_data = []\n",
    "        for item in data:\n",
    "            print(f\"Analyzing item: {item['title']}\")\n",
    "            text = self.preprocess(item[\"content\"])\n",
    "            entities = self.extract_entities(text)\n",
    "            keywords = self.extract_keywords(text)\n",
    "            summary = self.summarize(text)\n",
    "            paraphrased_content = self.paraphrase(summary)\n",
    "\n",
    "            processed_data.append({\n",
    "                \"title\": item[\"title\"],\n",
    "                \"url\": item[\"url\"],\n",
    "                \"entities\": entities,\n",
    "                \"keywords\": keywords,\n",
    "                \"summary\": summary,\n",
    "                \"paraphrased_content\": paraphrased_content\n",
    "            })\n",
    "\n",
    "        print(f\"Processed data: {processed_data}\")\n",
    "        return processed_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import openai\n",
    "\n",
    "class ReportGenerationModule:\n",
    "    def __init__(self):\n",
    "        self.api_key = \"CokHFBZqdMK1VfgaaUbiT3BlbkFJRCTVNL9WTgbwuAMks9kF\"\n",
    "        openai.api_key = self.api_key\n",
    "\n",
    "    def create_prompt(self, data_item):\n",
    "        # Add code to create a prompt based on the analyzed data\n",
    "        prompt = f\"{data_item['title']} - {data_item['paraphrased_content']}\"\n",
    "        print(f\"Created prompt: {prompt}\")\n",
    "        return prompt\n",
    "\n",
    "    def generate_text(self, prompt):\n",
    "        # Add code to generate text using the language model\n",
    "        response = openai.Completion.create(\n",
    "            engine=\"gpt-3.5-turbo\",\n",
    "            prompt=prompt,\n",
    "            max_tokens=3500,\n",
    "            n=1,\n",
    "            stop=None,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "\n",
    "        generated_text = response.choices[0].text.strip()\n",
    "        print(f\"Generated text: {generated_text}\")\n",
    "        return generated_text\n",
    "\n",
    "    def combine_sections(self, sections):\n",
    "        # Add code to combine the generated text into a structured report\n",
    "        report = \"\\n\".join(sections)\n",
    "        print(f\"Combined sections: {report}\")\n",
    "        return report\n",
    "\n",
    "    def add_citations(self, report, data_items):\n",
    "        # Add code to add citations and references to the report\n",
    "        citations = []\n",
    "        for i, data_item in enumerate(data_items, start=1):\n",
    "            citations.append(f\"[{i}] {data_item['url']}\")\n",
    "\n",
    "        report_with_citations = f\"{report}\\n\\nReferences:\\n\" + \"\\n\".join(citations)\n",
    "        print(f\"Report with citations: {report_with_citations}\")\n",
    "        return report_with_citations\n",
    "\n",
    "    def generate_report(self, analyzed_data):\n",
    "        report_sections = []\n",
    "        for data_item in analyzed_data:\n",
    "            print(f\"Generating report section for: {data_item['title']}\")\n",
    "            prompt = self.create_prompt(data_item)\n",
    "            generated_text = self.generate_text(prompt)\n",
    "            report_sections.append(generated_text)\n",
    "\n",
    "        report = self.combine_sections(report_sections)\n",
    "        report_with_citations = self.add_citations(report, analyzed_data)\n",
    "        print(f\"Generated report: {report_with_citations}\")\n",
    "        return report_with_citations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vladbordei/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import language_tool_python\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "class PostprocessorModule:\n",
    "    def __init__(self):\n",
    "        self.language_tool = language_tool_python.LanguageTool(\"en-US\")\n",
    "\n",
    "    def check_grammar_and_spelling(self, text):\n",
    "        # Add code to check grammar and spelling using LanguageTool\n",
    "        matches = self.language_tool.check(text)\n",
    "        print(f\"Grammar and spelling issues found: {matches}\")\n",
    "        return matches\n",
    "\n",
    "    def review_report(self, report):\n",
    "        # Add code to review the report, including grammar, spelling, and plagiarism checks\n",
    "        # This example focuses on grammar and spelling checks using LanguageTool\n",
    "        grammar_and_spelling_issues = self.check_grammar_and_spelling(report)\n",
    "        review_results = {\n",
    "            \"grammar_and_spelling\": grammar_and_spelling_issues\n",
    "        }\n",
    "        print(f\"Review results: {review_results}\")\n",
    "        return review_results\n",
    "\n",
    "    def generate_final_report(self, report, review_results):\n",
    "        # Add code to generate the final report by addressing any issues found in the review process\n",
    "        final_report = report\n",
    "        for issue in review_results[\"grammar_and_spelling\"]:\n",
    "            try:\n",
    "                suggested_correction = issue.replacements[0]\n",
    "                final_report = final_report.replace(issue.matched_text, suggested_correction)\n",
    "            except IndexError:\n",
    "                continue\n",
    "\n",
    "        print(f\"Final report: {final_report}\")\n",
    "        return final_report\n",
    "\n",
    "    def process_report(self, report):\n",
    "        print(\"Starting report postprocessing...\")\n",
    "        review_results = self.review_report(report)\n",
    "        final_report = self.generate_final_report(report, review_results)\n",
    "        print(\"Finished report postprocessing.\")\n",
    "        return final_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReportGenerator:\n",
    "    def __init__(self):\n",
    "        self.preprocessor = PreprocessorModule()\n",
    "        self.data_retrieval = DataRetrievalModule()\n",
    "        self.data_analysis = DataAnalysisModule()\n",
    "        self.report_generation = ReportGenerationModule()\n",
    "        self.postprocessor = PostprocessorModule()\n",
    "\n",
    "    def generate_search_queries(self, topic):\n",
    "        search_queries = self.preprocessor.generate_search_queries(topic)\n",
    "        return search_queries\n",
    "\n",
    "    def retrieve_data(self, search_queries):\n",
    "        retrieved_data = self.data_retrieval.retrieve_data(search_queries)\n",
    "        return retrieved_data\n",
    "\n",
    "    def analyze_data(self, retrieved_data):\n",
    "        analyzed_data = self.data_analysis.analyze_data(retrieved_data)\n",
    "        return analyzed_data\n",
    "\n",
    "    def generate_initial_report(self, analyzed_data):\n",
    "        initial_report = self.report_generation.generate_report(analyzed_data)\n",
    "        return initial_report\n",
    "\n",
    "    def process_report(self, initial_report):\n",
    "        final_report = self.postprocessor.process_report(initial_report)\n",
    "        return final_report\n",
    "\n",
    "    def generate_report(self, topic, additional_requirements=None):\n",
    "        search_queries = self.generate_search_queries(topic)\n",
    "        retrieved_data = self.retrieve_data(search_queries)\n",
    "        analyzed_data = self.analyze_data(retrieved_data)\n",
    "        initial_report = self.generate_initial_report(analyzed_data)\n",
    "        final_report = self.process_report(initial_report)\n",
    "\n",
    "        return final_report\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "report_generator = ReportGenerator()\n",
    "\n",
    "topic = \"Natural Language Processing\"\n",
    "search_queries = report_generator.generate_search_queries(topic)\n",
    "retrieved_data = report_generator.retrieve_data(search_queries)\n",
    "analyzed_data = report_generator.analyze_data(retrieved_data)\n",
    "initial_report = report_generator.generate_initial_report(analyzed_data)\n",
    "final_report = report_generator.process_report(initial_report)\n",
    "\n",
    "print(final_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "report_generator = ReportGenerator()\n",
    "topic = \"Integrating Battery Storage Systems with Renewable Energy Sources on Site\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Lowercase topic: integrating battery storage systems with renewable energy sources on site\n",
      "Step 2: POS tags: [('integrating', 'VBG'), ('battery', 'NN'), ('storage', 'NN'), ('systems', 'NNS'), ('with', 'IN'), ('renewable', 'JJ'), ('energy', 'NN'), ('sources', 'NNS'), ('on', 'IN'), ('site', 'NN')]\n",
      "Step 4: Filtered tokens (stopwords removed): ['integrating', 'battery', 'storage', 'systems', 'renewable', 'energy', 'sources', 'site']\n",
      "Step 5: Stemmed tokens: ['integr', 'batteri', 'storag', 'system', 'renew', 'energi', 'sourc', 'site']\n",
      "Step 6: Search query: integrating battery storage systems renewable energy sources site\n"
     ]
    }
   ],
   "source": [
    "search_queries = report_generator.generate_search_queries(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oaie",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
